[{"id":"3806f23b-478c-4054-b6c1-37f11db58d38","displayName":"Read a DataFrame from Hive","description":"This step will read a dataFrame in a given format from Hive","type":"Pipeline","category":"InputOutput","params":[{"type":"text","name":"table","required":false,"parameterType":"String"},{"type":"object","name":"options","required":false,"className":"com.acxiom.pipeline.steps.DataFrameReaderOptions","parameterType":"com.acxiom.pipeline.steps.DataFrameReaderOptions"}],"engineMeta":{"spark":"HiveSteps.readDataFrame","pkg":"com.acxiom.pipeline.steps","results":{"primaryType":"org.apache.spark.sql.DataFrame"}},"creationDate":"2019-10-24T11:29:07.380Z","modifiedDate":"2019-10-24T11:29:07.380Z"},{"id":"e2b4c011-e71b-46f9-a8be-cf937abc2ec4","displayName":"Write DataFrame to Hive","description":"This step will write a dataFrame in a given format to Hive","type":"Pipeline","category":"InputOutput","params":[{"type":"text","name":"dataFrame","required":false,"parameterType":"org.apache.spark.sql.DataFrame"},{"type":"text","name":"table","required":false,"parameterType":"String"},{"type":"object","name":"options","required":false,"className":"com.acxiom.pipeline.steps.DataFrameWriterOptions","parameterType":"com.acxiom.pipeline.steps.DataFrameWriterOptions"}],"engineMeta":{"spark":"HiveSteps.writeDataFrame","pkg":"com.acxiom.pipeline.steps"},"creationDate":"2019-10-24T11:29:07.391Z","modifiedDate":"2019-10-24T11:29:07.391Z"},{"id":"87db259d-606e-46eb-b723-82923349640f","displayName":"Load DataFrame from HDFS path","description":"This step will read a dataFrame from the given HDFS path","type":"Pipeline","category":"InputOutput","params":[{"type":"text","name":"path","required":false,"parameterType":"String"},{"type":"object","name":"options","required":false,"className":"com.acxiom.pipeline.steps.DataFrameReaderOptions","parameterType":"com.acxiom.pipeline.steps.DataFrameReaderOptions"}],"engineMeta":{"spark":"HDFSSteps.readFromPath","pkg":"com.acxiom.pipeline.steps","results":{"primaryType":"org.apache.spark.sql.DataFrame"}},"creationDate":"2019-10-24T11:29:07.395Z","modifiedDate":"2019-10-24T11:29:07.395Z"},{"id":"8daea683-ecde-44ce-988e-41630d251cb8","displayName":"Load DataFrame from HDFS paths","description":"This step will read a dataFrame from the given HDFS paths","type":"Pipeline","category":"InputOutput","params":[{"type":"text","name":"paths","required":false,"parameterType":"List[String]"},{"type":"object","name":"options","required":false,"className":"com.acxiom.pipeline.steps.DataFrameReaderOptions","parameterType":"com.acxiom.pipeline.steps.DataFrameReaderOptions"}],"engineMeta":{"spark":"HDFSSteps.readFromPaths","pkg":"com.acxiom.pipeline.steps","results":{"primaryType":"org.apache.spark.sql.DataFrame"}},"creationDate":"2019-10-24T11:29:07.401Z","modifiedDate":"2019-10-24T11:29:07.401Z"},{"id":"0a296858-e8b7-43dd-9f55-88d00a7cd8fa","displayName":"Write DataFrame to HDFS","description":"This step will write a dataFrame in a given format to HDFS","type":"Pipeline","category":"InputOutput","params":[{"type":"text","name":"dataFrame","required":false,"parameterType":"org.apache.spark.sql.DataFrame"},{"type":"text","name":"path","required":false,"parameterType":"String"},{"type":"object","name":"options","required":false,"className":"com.acxiom.pipeline.steps.DataFrameWriterOptions","parameterType":"com.acxiom.pipeline.steps.DataFrameWriterOptions"}],"engineMeta":{"spark":"HDFSSteps.writeToPath","pkg":"com.acxiom.pipeline.steps"},"creationDate":"2019-10-24T11:29:07.406Z","modifiedDate":"2019-10-24T11:29:07.406Z"},{"id":"e4dad367-a506-5afd-86c0-82c2cf5cd15c","displayName":"Create HDFS FileManager","description":"Simple function to generate the HDFSFileManager for the local HDFS file system","type":"Pipeline","category":"InputOutput","params":[],"engineMeta":{"spark":"HDFSSteps.createFileManager","pkg":"com.acxiom.pipeline.steps","results":{"primaryType":"com.acxiom.pipeline.fs.HDFSFileManager"}},"creationDate":"2019-10-24T11:29:07.408Z","modifiedDate":"2019-10-24T11:29:07.408Z"},{"id":"a7e17c9d-6956-4be0-a602-5b5db4d1c08b","displayName":"Scala script Step","description":"Executes a script and returns the result","type":"Pipeline","category":"Scripting","params":[{"type":"script","name":"script","required":false,"language":"scala","className":"String"}],"engineMeta":{"spark":"ScalaSteps.processScript","pkg":"com.acxiom.pipeline.steps","results":{"primaryType":"com.acxiom.pipeline.PipelineStepResponse"}},"creationDate":"2019-10-24T11:29:07.413Z","modifiedDate":"2019-10-24T11:29:07.413Z"},{"id":"8bf8cef6-cf32-4d85-99f4-e4687a142f84","displayName":"Scala script Step with additional object provided","description":"Executes a script with the provided object and returns the result","type":"Pipeline","category":"Scripting","params":[{"type":"script","name":"script","required":false,"language":"scala","className":"String"},{"type":"text","name":"value","required":false,"parameterType":"Any"},{"type":"text","name":"type","required":false,"parameterType":"String"}],"engineMeta":{"spark":"ScalaSteps.processScriptWithValue","pkg":"com.acxiom.pipeline.steps","results":{"primaryType":"com.acxiom.pipeline.PipelineStepResponse"}},"creationDate":"2019-10-24T11:29:07.419Z","modifiedDate":"2019-10-24T11:29:07.419Z"},{"id":"cdb332e3-9ea4-4c96-8b29-c1d74287656c","displayName":"Load table as DataFrame using JDBCOptions","description":"This step will load a table from the provided JDBCOptions","type":"Pipeline","category":"InputOutput","params":[{"type":"text","name":"jdbcOptions","required":false,"parameterType":"org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions"}],"engineMeta":{"spark":"JDBCSteps.readWithJDBCOptions","pkg":"com.acxiom.pipeline.steps","results":{"primaryType":"org.apache.spark.sql.DataFrame"}},"creationDate":"2019-10-24T11:29:07.422Z","modifiedDate":"2019-10-24T11:29:07.422Z"},{"id":"72dbbfc8-bd1d-4ce4-ab35-28fa8385ea54","displayName":"Load table as DataFrame using StepOptions","description":"This step will load a table from the provided JDBCDataFrameReaderOptions","type":"Pipeline","category":"InputOutput","params":[{"type":"object","name":"jDBCStepsOptions","required":false,"className":"com.acxiom.pipeline.steps.JDBCDataFrameReaderOptions","parameterType":"com.acxiom.pipeline.steps.JDBCDataFrameReaderOptions"}],"engineMeta":{"spark":"JDBCSteps.readWithStepOptions","pkg":"com.acxiom.pipeline.steps","results":{"primaryType":"org.apache.spark.sql.DataFrame"}},"creationDate":"2019-10-24T11:29:07.423Z","modifiedDate":"2019-10-24T11:29:07.423Z"},{"id":"dcc57409-eb91-48c0-975b-ca109ba30195","displayName":"Load table as DataFrame","description":"This step will load a table from the provided jdbc information","type":"Pipeline","category":"InputOutput","params":[{"type":"text","name":"url","required":false,"parameterType":"String"},{"type":"text","name":"table","required":false,"parameterType":"String"},{"type":"text","name":"predicates","required":false,"parameterType":"List[String]"},{"type":"text","name":"connectionProperties","required":false,"parameterType":"Map[String,String]"}],"engineMeta":{"spark":"JDBCSteps.readWithProperties","pkg":"com.acxiom.pipeline.steps","results":{"primaryType":"org.apache.spark.sql.DataFrame"}},"creationDate":"2019-10-24T11:29:07.425Z","modifiedDate":"2019-10-24T11:29:07.425Z"},{"id":"c9fddf52-34b1-4216-a049-10c33ccd24ab","displayName":"Write DataFrame to table using JDBCOptions","description":"This step will write a DataFrame as a table using JDBCOptions","type":"Pipeline","category":"InputOutput","params":[{"type":"text","name":"dataFrame","required":false,"parameterType":"org.apache.spark.sql.DataFrame"},{"type":"text","name":"jdbcOptions","required":false,"parameterType":"org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions"},{"type":"text","name":"saveMode","required":false,"parameterType":"String"}],"engineMeta":{"spark":"JDBCSteps.writeWithJDBCOptions","pkg":"com.acxiom.pipeline.steps"},"creationDate":"2019-10-24T11:29:07.428Z","modifiedDate":"2019-10-24T11:29:07.428Z"},{"id":"77ffcd02-fbd0-4f79-9b35-ac9dc5fb7190","displayName":"Write DataFrame to table","description":"This step will write a DataFrame to a table using the provided properties","type":"Pipeline","category":"InputOutput","params":[{"type":"text","name":"dataFrame","required":false,"parameterType":"org.apache.spark.sql.DataFrame"},{"type":"text","name":"url","required":false,"parameterType":"String"},{"type":"text","name":"table","required":false,"parameterType":"String"},{"type":"text","name":"connectionProperties","required":false,"parameterType":"Map[String,String]"},{"type":"text","name":"saveMode","required":false,"parameterType":"String"}],"engineMeta":{"spark":"JDBCSteps.writeWithProperties","pkg":"com.acxiom.pipeline.steps"},"creationDate":"2019-10-24T11:29:07.432Z","modifiedDate":"2019-10-24T11:29:07.432Z"},{"id":"3d6b77a1-52c2-49ba-99a0-7ec773dac696","displayName":"Write DataFrame to JDBC table","description":"This step will write a DataFrame to a table using the provided JDBCDataFrameWriterOptions","type":"Pipeline","category":"InputOutput","params":[{"type":"text","name":"dataFrame","required":false,"parameterType":"org.apache.spark.sql.DataFrame"},{"type":"object","name":"jDBCStepsOptions","required":false,"className":"com.acxiom.pipeline.steps.JDBCDataFrameWriterOptions","parameterType":"com.acxiom.pipeline.steps.JDBCDataFrameWriterOptions"}],"engineMeta":{"spark":"JDBCSteps.writeWithStepOptions","pkg":"com.acxiom.pipeline.steps"},"creationDate":"2019-10-24T11:29:07.437Z","modifiedDate":"2019-10-24T11:29:07.437Z"},{"id":"219c787a-f502-4efc-b15d-5beeff661fc0","displayName":"Map a DataFrame to an existing DataFrame","description":"This step maps a new dataframe to an existing dataframe to make them compatible","type":"Pipeline","category":"Transforms","params":[{"type":"text","name":"inputDataFrame","required":false,"parameterType":"org.apache.spark.sql.DataFrame"},{"type":"text","name":"destinationDataFrame","required":false,"parameterType":"org.apache.spark.sql.DataFrame"},{"type":"object","name":"transforms","required":false,"className":"com.acxiom.pipeline.steps.Transformations","parameterType":"com.acxiom.pipeline.steps.Transformations"},{"type":"boolean","name":"addNewColumns","required":false,"parameterType":"Boolean"}],"engineMeta":{"spark":"TransformationSteps.mapToDestinationDataFrame","pkg":"com.acxiom.pipeline.steps","results":{"primaryType":"org.apache.spark.sql.DataFrame"}},"creationDate":"2019-10-24T11:29:07.439Z","modifiedDate":"2019-10-24T11:29:07.439Z"},{"id":"8f9c08ea-4882-4265-bac7-2da3e942758f","displayName":"Map a DataFrame to a pre-defined Schema","description":"This step maps a new dataframe to a pre-defined spark schema","type":"Pipeline","category":"Transforms","params":[{"type":"text","name":"inputDataFrame","required":false,"parameterType":"org.apache.spark.sql.DataFrame"},{"type":"object","name":"destinationSchema","required":false,"className":"com.acxiom.pipeline.steps.Schema","parameterType":"com.acxiom.pipeline.steps.Schema"},{"type":"object","name":"transforms","required":false,"className":"com.acxiom.pipeline.steps.Transformations","parameterType":"com.acxiom.pipeline.steps.Transformations"},{"type":"boolean","name":"addNewColumns","required":false,"parameterType":"Boolean"}],"engineMeta":{"spark":"TransformationSteps.mapDataFrameToSchema","pkg":"com.acxiom.pipeline.steps","results":{"primaryType":"org.apache.spark.sql.DataFrame"}},"creationDate":"2019-10-24T11:29:07.440Z","modifiedDate":"2019-10-24T11:29:07.440Z"},{"id":"3ee74590-9131-43e1-8ee8-ad320482a592","displayName":"Merge a DataFrame to an existing DataFrame","description":"This step merges two dataframes to create a single dataframe","type":"Pipeline","category":"Transforms","params":[{"type":"text","name":"inputDataFrame","required":false,"parameterType":"org.apache.spark.sql.DataFrame"},{"type":"text","name":"destinationDataFrame","required":false,"parameterType":"org.apache.spark.sql.DataFrame"},{"type":"object","name":"transforms","required":false,"className":"com.acxiom.pipeline.steps.Transformations","parameterType":"com.acxiom.pipeline.steps.Transformations"},{"type":"boolean","name":"addNewColumns","required":false,"parameterType":"Boolean"}],"engineMeta":{"spark":"TransformationSteps.mergeDataFrames","pkg":"com.acxiom.pipeline.steps","results":{"primaryType":"org.apache.spark.sql.DataFrame"}},"creationDate":"2019-10-24T11:29:07.442Z","modifiedDate":"2019-10-24T11:29:07.442Z"},{"id":"ac3dafe4-e6ee-45c9-8fc6-fa7f918cf4f2","displayName":"Modify or Create Columns using Transforms Provided","description":"This step transforms existing columns and/or adds new columns to an existing dataframe using expressions provided","type":"Pipeline","category":"Transforms","params":[{"type":"text","name":"dataFrame","required":false,"parameterType":"org.apache.spark.sql.DataFrame"},{"type":"object","name":"transforms","required":false,"className":"com.acxiom.pipeline.steps.Transformations","parameterType":"com.acxiom.pipeline.steps.Transformations"}],"engineMeta":{"spark":"TransformationSteps.applyTransforms","pkg":"com.acxiom.pipeline.steps","results":{"primaryType":"org.apache.spark.sql.DataFrame"}},"creationDate":"2019-10-24T11:29:07.445Z","modifiedDate":"2019-10-24T11:29:07.445Z"},{"id":"fa0fcabb-d000-4a5e-9144-692bca618ddb","displayName":"Filter a DataFrame","description":"This step will filter a dataframe based on the where expression provided","type":"Pipeline","category":"Transforms","params":[{"type":"text","name":"dataFrame","required":false,"parameterType":"org.apache.spark.sql.DataFrame"},{"type":"text","name":"expression","required":false,"parameterType":"String"}],"engineMeta":{"spark":"TransformationSteps.applyFilter","pkg":"com.acxiom.pipeline.steps","results":{"primaryType":"org.apache.spark.sql.DataFrame"}},"creationDate":"2019-10-24T11:29:07.447Z","modifiedDate":"2019-10-24T11:29:07.447Z"},{"id":"a981080d-714c-4d36-8b09-d95842ec5655","displayName":"Standardize Column Names on a DataFrame","description":"This step will standardize columns names on existing dataframe","type":"Pipeline","category":"Transforms","params":[{"type":"text","name":"dataFrame","required":false,"parameterType":"org.apache.spark.sql.DataFrame"}],"engineMeta":{"spark":"TransformationSteps.standardizeColumnNames","pkg":"com.acxiom.pipeline.steps","results":{"primaryType":"org.apache.spark.sql.DataFrame"}},"creationDate":"2019-10-24T11:29:07.453Z","modifiedDate":"2019-10-24T11:29:07.453Z"},{"id":"541c4f7d-3524-4d53-bbd9-9f2cfd9d1bd1","displayName":"Save a Dataframe to a TempView","description":"This step stores an existing dataframe to a TempView to be used in future queries in the session","type":"Pipeline","category":"Query","params":[{"type":"text","name":"dataFrame","required":false,"parameterType":"org.apache.spark.sql.DataFrame"},{"type":"text","name":"viewName","required":false,"parameterType":"String"}],"engineMeta":{"spark":"QuerySteps.dataFrameToTempView","pkg":"com.acxiom.pipeline.steps","results":{"primaryType":"String"}},"creationDate":"2019-10-24T11:29:07.456Z","modifiedDate":"2019-10-24T11:29:07.456Z"},{"id":"71b71ef3-eaa7-4a1f-b3f3-603a1a54846d","displayName":"Create a TempView from a Query","description":"This step runs a SQL statement against existing TempViews from this session and returns a new TempView","type":"Pipeline","category":"Query","params":[{"type":"script","name":"query","required":false,"language":"sql","className":"String"},{"type":"text","name":"variableMap","required":false,"parameterType":"Map[String,String]"},{"type":"text","name":"viewName","required":false,"parameterType":"String"}],"engineMeta":{"spark":"QuerySteps.queryToTempView","pkg":"com.acxiom.pipeline.steps","results":{"primaryType":"String"}},"creationDate":"2019-10-24T11:29:07.458Z","modifiedDate":"2019-10-24T11:29:07.458Z"},{"id":"61378ed6-8a4f-4e6d-9c92-6863c9503a54","displayName":"Create a DataFrame from a Query","description":"This step runs a SQL statement against existing TempViews from this session and returns a new DataFrame","type":"Pipeline","category":"Query","params":[{"type":"script","name":"query","required":false,"language":"sql","className":"String"},{"type":"text","name":"variableMap","required":false,"parameterType":"Map[String,String]"}],"engineMeta":{"spark":"QuerySteps.queryToDataFrame","pkg":"com.acxiom.pipeline.steps","results":{"primaryType":"org.apache.spark.sql.DataFrame"}},"creationDate":"2019-10-24T11:29:07.461Z","modifiedDate":"2019-10-24T11:29:07.461Z"},{"id":"57b0e491-e09b-4428-aab2-cebe1f217eda","displayName":"Create a DataFrame from an Existing TempView","description":"This step pulls an existing TempView from this session into a new DataFrame","type":"Pipeline","category":"Query","params":[{"type":"text","name":"viewName","required":false,"parameterType":"String"}],"engineMeta":{"spark":"QuerySteps.tempViewToDataFrame","pkg":"com.acxiom.pipeline.steps","results":{"primaryType":"org.apache.spark.sql.DataFrame"}},"creationDate":"2019-10-24T11:29:07.463Z","modifiedDate":"2019-10-24T11:29:07.463Z"},{"id":"648f27aa-6e3b-44ed-a093-bc284783731b","displayName":"Create a TempView from a DataFrame Query","description":"This step runs a SQL statement against an existing DataFrame from this session and returns a new TempView","type":"Pipeline","category":"Query","params":[{"type":"text","name":"dataFrame","required":false,"parameterType":"org.apache.spark.sql.DataFrame"},{"type":"script","name":"query","required":false,"language":"sql","className":"String"},{"type":"text","name":"variableMap","required":false,"parameterType":"Map[String,String]"},{"type":"text","name":"inputViewName","required":false,"parameterType":"String"},{"type":"text","name":"outputViewName","required":false,"parameterType":"String"}],"engineMeta":{"spark":"QuerySteps.dataFrameQueryToTempView","pkg":"com.acxiom.pipeline.steps","results":{"primaryType":"String"}},"creationDate":"2019-10-24T11:29:07.471Z","modifiedDate":"2019-10-24T11:29:07.471Z"},{"id":"dfb8a387-6245-4b1c-ae6c-94067eb83962","displayName":"Create a DataFrame from a DataFrame Query","description":"This step runs a SQL statement against an existing DataFrame from this session and returns a new DataFrame","type":"Pipeline","category":"Query","params":[{"type":"text","name":"dataFrame","required":false,"parameterType":"org.apache.spark.sql.DataFrame"},{"type":"script","name":"query","required":false,"language":"sql","className":"String"},{"type":"text","name":"variableMap","required":false,"parameterType":"Map[String,String]"},{"type":"text","name":"inputViewName","required":false,"parameterType":"String"}],"engineMeta":{"spark":"QuerySteps.dataFrameQueryToDataFrame","pkg":"com.acxiom.pipeline.steps","results":{"primaryType":"org.apache.spark.sql.DataFrame"}},"creationDate":"2019-10-24T11:29:07.473Z","modifiedDate":"2019-10-24T11:29:07.473Z"},{"id":"c88de095-14e0-4c67-8537-0325127e2bd2","displayName":"Cache an exising TempView","description":"This step will cache an existing TempView","type":"Pipeline","category":"Query","params":[{"type":"text","name":"viewName","required":false,"parameterType":"String"}],"engineMeta":{"spark":"QuerySteps.cacheTempView","pkg":"com.acxiom.pipeline.steps","results":{"primaryType":"org.apache.spark.sql.DataFrame"}},"creationDate":"2019-10-24T11:29:07.477Z","modifiedDate":"2019-10-24T11:29:07.477Z"},{"id":"0342654c-2722-56fe-ba22-e342169545af","displayName":"Copy source contents to destination","description":"Copy the contents of the source path to the destination path. This function will call connect on both FileManagers.","type":"Pipeline","category":"InputOutput","params":[{"type":"text","name":"srcFS","required":false,"parameterType":"com.acxiom.pipeline.fs.FileManager"},{"type":"text","name":"srcPath","required":false,"parameterType":"String"},{"type":"text","name":"destFS","required":false,"parameterType":"com.acxiom.pipeline.fs.FileManager"},{"type":"text","name":"destPath","required":false,"parameterType":"String"}],"engineMeta":{"spark":"FileManagerSteps.copy","pkg":"com.acxiom.pipeline.steps","results":{"primaryType":"com.acxiom.pipeline.steps.CopyResults"}},"creationDate":"2019-10-24T11:29:07.483Z","modifiedDate":"2019-10-24T11:29:07.483Z"},{"id":"c40169a3-1e77-51ab-9e0a-3f24fb98beef","displayName":"Copy source contents to destination with buffering","description":"Copy the contents of the source path to the destination path using buffer sizes. This function will call connect on both FileManagers.","type":"Pipeline","category":"InputOutput","params":[{"type":"text","name":"srcFS","required":false,"parameterType":"com.acxiom.pipeline.fs.FileManager"},{"type":"text","name":"srcPath","required":false,"parameterType":"String"},{"type":"text","name":"destFS","required":false,"parameterType":"com.acxiom.pipeline.fs.FileManager"},{"type":"text","name":"destPath","required":false,"parameterType":"String"},{"type":"text","name":"inputBufferSize","required":false,"parameterType":"Int"},{"type":"text","name":"outputBufferSize","required":false,"parameterType":"Int"}],"engineMeta":{"spark":"FileManagerSteps.copy","pkg":"com.acxiom.pipeline.steps","results":{"primaryType":"com.acxiom.pipeline.steps.CopyResults"}},"creationDate":"2019-10-24T11:29:07.488Z","modifiedDate":"2019-10-24T11:29:07.488Z"},{"id":"f5a24db0-e91b-5c88-8e67-ab5cff09c883","displayName":"Buffered file copy","description":"Copy the contents of the source path to the destination path using full buffer sizes. This function will call connect on both FileManagers.","type":"Pipeline","category":"InputOutput","params":[{"type":"text","name":"srcFS","required":false,"parameterType":"com.acxiom.pipeline.fs.FileManager"},{"type":"text","name":"srcPath","required":false,"parameterType":"String"},{"type":"text","name":"destFS","required":false,"parameterType":"com.acxiom.pipeline.fs.FileManager"},{"type":"text","name":"destPath","required":false,"parameterType":"String"},{"type":"text","name":"inputBufferSize","required":false,"parameterType":"Int"},{"type":"text","name":"outputBufferSize","required":false,"parameterType":"Int"},{"type":"text","name":"copyBufferSize","required":false,"parameterType":"Int"}],"engineMeta":{"spark":"FileManagerSteps.copy","pkg":"com.acxiom.pipeline.steps","results":{"primaryType":"com.acxiom.pipeline.steps.CopyResults"}},"creationDate":"2019-10-24T11:29:07.491Z","modifiedDate":"2019-10-24T11:29:07.491Z"},{"id":"3d1e8519-690c-55f0-bd05-1e7b97fb6633","displayName":"Disconnect a FileManager","description":"Disconnects a FileManager from the underlying file system","type":"Pipeline","category":"InputOutput","params":[{"type":"text","name":"fileManager","required":false,"parameterType":"com.acxiom.pipeline.fs.FileManager"}],"engineMeta":{"spark":"FileManagerSteps.disconnectFileManager","pkg":"com.acxiom.pipeline.steps"},"creationDate":"2019-10-24T11:29:07.495Z","modifiedDate":"2019-10-24T11:29:07.495Z"},{"id":"9d467cb0-8b3d-40a0-9ccd-9cf8c5b6cb38","displayName":"Create SFTP FileManager","description":"Simple function to generate the SFTPFileManager for the remote SFTP file system","type":"Pipeline","category":"InputOutput","params":[{"type":"text","name":"hostName","required":false,"parameterType":"String"},{"type":"text","name":"username","required":false,"parameterType":"String"},{"type":"text","name":"password","required":false,"parameterType":"String"},{"type":"text","name":"port","required":false,"parameterType":"Int"},{"type":"boolean","name":"strictHostChecking","required":false,"parameterType":"Boolean"}],"engineMeta":{"spark":"SFTPSteps.createFileManager","pkg":"com.acxiom.pipeline.steps","results":{"primaryType":"com.acxiom.pipeline.fs.SFTPFileManager"}},"creationDate":"2019-10-24T11:29:07.501Z","modifiedDate":"2019-10-24T11:29:07.501Z"},{"id":"22fcc0e7-0190-461c-a999-9116b77d5919","displayName":"Build a DataFrameReader Object","description":"This step will build a DataFrameReader object that can be used to read a file into a dataframe","type":"Pipeline","category":"InputOutput","params":[{"type":"object","name":"dataFrameReaderOptions","required":false,"className":"com.acxiom.pipeline.steps.DataFrameReaderOptions","parameterType":"com.acxiom.pipeline.steps.DataFrameReaderOptions"}],"engineMeta":{"spark":"DataFrameSteps.getDataFrameReader","pkg":"com.acxiom.pipeline.steps","results":{"primaryType":"org.apache.spark.sql.DataFrameReader"}},"creationDate":"2019-10-24T11:29:07.505Z","modifiedDate":"2019-10-24T11:29:07.505Z"},{"id":"e023fc14-6cb7-44cb-afce-7de01d5cdf00","displayName":"Build a DataFrameWriter Object","description":"This step will build a DataFrameWriter object that can be used to write a file into a dataframe","type":"Pipeline","category":"InputOutput","params":[{"type":"text","name":"dataFrame","required":false,"parameterType":"org.apache.spark.sql.DataFrame"},{"type":"object","name":"options","required":false,"className":"com.acxiom.pipeline.steps.DataFrameWriterOptions","parameterType":"com.acxiom.pipeline.steps.DataFrameWriterOptions"}],"engineMeta":{"spark":"DataFrameSteps.getDataFrameWriter","pkg":"com.acxiom.pipeline.steps","results":{"primaryType":"org.apache.spark.sql.DataFrameWriter[org.apache.spark.sql.Row]"}},"creationDate":"2019-10-24T11:29:07.507Z","modifiedDate":"2019-10-24T11:29:07.507Z"},{"id":"5e0358a0-d567-5508-af61-c35a69286e4e","displayName":"Javascript Step","description":"Executes a script and returns the result","type":"Pipeline","category":"Scripting","params":[{"type":"script","name":"script","required":false,"language":"javascript","className":"String"}],"engineMeta":{"spark":"JavascriptSteps.processScript","pkg":"com.acxiom.pipeline.steps","results":{"primaryType":"com.acxiom.pipeline.PipelineStepResponse"}},"creationDate":"2019-10-24T11:29:07.510Z","modifiedDate":"2019-10-24T11:29:07.510Z"},{"id":"570c9a80-8bd1-5f0c-9ae0-605921fe51e2","displayName":"Javascript Step with additional object provided","description":"Executes a script and returns the result","type":"Pipeline","category":"Scripting","params":[{"type":"script","name":"script","required":false,"language":"javascript","className":"String"},{"type":"text","name":"value","required":false,"parameterType":"Any"}],"engineMeta":{"spark":"JavascriptSteps.processScriptWithValue","pkg":"com.acxiom.pipeline.steps","results":{"primaryType":"com.acxiom.pipeline.PipelineStepResponse"}},"creationDate":"2019-10-24T11:29:07.514Z","modifiedDate":"2019-10-24T11:29:07.514Z"}]