{"_id":{"$oid":"5dc167fef8d649ecb358d982"},"id":"3806f23b-478c-4054-b6c1-37f11db58d38","category":"InputOutput","creationDate":{"$date":"2019-11-05T12:15:58.143Z"},"description":"This step will read a dataFrame in a given format from Hive","displayName":"Read a DataFrame from Hive","engineMeta":{"spark":"HiveSteps.readDataFrame","pkg":"com.acxiom.pipeline.steps","results":{"primaryType":"org.apache.spark.sql.DataFrame"}},"modifiedDate":{"$date":"2019-11-05T12:15:58.143Z"},"params":[{"type":"text","name":"table","required":false,"parameterType":"String"},{"type":"object","name":"options","required":false,"className":"com.acxiom.pipeline.steps.DataFrameReaderOptions","parameterType":"com.acxiom.pipeline.steps.DataFrameReaderOptions"}],"type":"Pipeline"}
{"_id":{"$oid":"5dc167fef8d649ecb358d989"},"id":"e2b4c011-e71b-46f9-a8be-cf937abc2ec4","category":"InputOutput","creationDate":{"$date":"2019-11-05T12:15:58.233Z"},"description":"This step will write a dataFrame in a given format to Hive","displayName":"Write DataFrame to Hive","engineMeta":{"spark":"HiveSteps.writeDataFrame","pkg":"com.acxiom.pipeline.steps"},"modifiedDate":{"$date":"2019-11-05T12:15:58.233Z"},"params":[{"type":"text","name":"dataFrame","required":false,"parameterType":"org.apache.spark.sql.DataFrame"},{"type":"text","name":"table","required":false,"parameterType":"String"},{"type":"object","name":"options","required":false,"className":"com.acxiom.pipeline.steps.DataFrameWriterOptions","parameterType":"com.acxiom.pipeline.steps.DataFrameWriterOptions"}],"type":"Pipeline"}
{"_id":{"$oid":"5dc167fef8d649ecb358d991"},"id":"87db259d-606e-46eb-b723-82923349640f","category":"InputOutput","creationDate":{"$date":"2019-11-05T12:15:58.238Z"},"description":"This step will read a dataFrame from the given HDFS path","displayName":"Load DataFrame from HDFS path","engineMeta":{"spark":"HDFSSteps.readFromPath","pkg":"com.acxiom.pipeline.steps","results":{"primaryType":"org.apache.spark.sql.DataFrame"}},"modifiedDate":{"$date":"2019-11-05T12:15:58.238Z"},"params":[{"type":"text","name":"path","required":false,"parameterType":"String"},{"type":"object","name":"options","required":false,"className":"com.acxiom.pipeline.steps.DataFrameReaderOptions","parameterType":"com.acxiom.pipeline.steps.DataFrameReaderOptions"}],"type":"Pipeline"}
{"_id":{"$oid":"5dc167fef8d649ecb358d999"},"id":"8daea683-ecde-44ce-988e-41630d251cb8","category":"InputOutput","creationDate":{"$date":"2019-11-05T12:15:58.246Z"},"description":"This step will read a dataFrame from the given HDFS paths","displayName":"Load DataFrame from HDFS paths","engineMeta":{"spark":"HDFSSteps.readFromPaths","pkg":"com.acxiom.pipeline.steps","results":{"primaryType":"org.apache.spark.sql.DataFrame"}},"modifiedDate":{"$date":"2019-11-05T12:15:58.246Z"},"params":[{"type":"text","name":"paths","required":false,"parameterType":"List[String]"},{"type":"object","name":"options","required":false,"className":"com.acxiom.pipeline.steps.DataFrameReaderOptions","parameterType":"com.acxiom.pipeline.steps.DataFrameReaderOptions"}],"type":"Pipeline"}
{"_id":{"$oid":"5dc167fef8d649ecb358d9a1"},"id":"0a296858-e8b7-43dd-9f55-88d00a7cd8fa","category":"InputOutput","creationDate":{"$date":"2019-11-05T12:15:58.254Z"},"description":"This step will write a dataFrame in a given format to HDFS","displayName":"Write DataFrame to HDFS","engineMeta":{"spark":"HDFSSteps.writeToPath","pkg":"com.acxiom.pipeline.steps"},"modifiedDate":{"$date":"2019-11-05T12:15:58.254Z"},"params":[{"type":"text","name":"dataFrame","required":false,"parameterType":"org.apache.spark.sql.DataFrame"},{"type":"text","name":"path","required":false,"parameterType":"String"},{"type":"object","name":"options","required":false,"className":"com.acxiom.pipeline.steps.DataFrameWriterOptions","parameterType":"com.acxiom.pipeline.steps.DataFrameWriterOptions"}],"type":"Pipeline"}
{"_id":{"$oid":"5dc167fef8d649ecb358d9a9"},"id":"e4dad367-a506-5afd-86c0-82c2cf5cd15c","category":"InputOutput","creationDate":{"$date":"2019-11-05T12:15:58.258Z"},"description":"Simple function to generate the HDFSFileManager for the local HDFS file system","displayName":"Create HDFS FileManager","engineMeta":{"spark":"HDFSSteps.createFileManager","pkg":"com.acxiom.pipeline.steps","results":{"primaryType":"com.acxiom.pipeline.fs.HDFSFileManager"}},"modifiedDate":{"$date":"2019-11-05T12:15:58.258Z"},"params":[],"type":"Pipeline"}
{"_id":{"$oid":"5dc167fef8d649ecb358d9af"},"id":"a7e17c9d-6956-4be0-a602-5b5db4d1c08b","category":"Scripting","creationDate":{"$date":"2019-11-05T12:15:58.265Z"},"description":"Executes a script and returns the result","displayName":"Scala script Step","engineMeta":{"spark":"ScalaSteps.processScript","pkg":"com.acxiom.pipeline.steps","results":{"primaryType":"com.acxiom.pipeline.PipelineStepResponse"}},"modifiedDate":{"$date":"2019-11-05T12:15:58.265Z"},"params":[{"type":"script","name":"script","required":false,"language":"scala","className":"String"}],"type":"Pipeline"}
{"_id":{"$oid":"5dc167fef8d649ecb358d9b3"},"id":"8bf8cef6-cf32-4d85-99f4-e4687a142f84","category":"Scripting","creationDate":{"$date":"2019-11-05T12:15:58.269Z"},"description":"Executes a script with the provided object and returns the result","displayName":"Scala script Step with additional object provided","engineMeta":{"spark":"ScalaSteps.processScriptWithValue","pkg":"com.acxiom.pipeline.steps","results":{"primaryType":"com.acxiom.pipeline.PipelineStepResponse"}},"modifiedDate":{"$date":"2019-11-05T12:15:58.269Z"},"params":[{"type":"script","name":"script","required":false,"language":"scala","className":"String"},{"type":"text","name":"value","required":false,"parameterType":"Any"},{"type":"text","name":"type","required":false,"parameterType":"String"}],"type":"Pipeline"}
{"_id":{"$oid":"5dc167fef8d649ecb358d9b8"},"id":"cdb332e3-9ea4-4c96-8b29-c1d74287656c","category":"InputOutput","creationDate":{"$date":"2019-11-05T12:15:58.274Z"},"description":"This step will load a table from the provided JDBCOptions","displayName":"Load table as DataFrame using JDBCOptions","engineMeta":{"spark":"JDBCSteps.readWithJDBCOptions","pkg":"com.acxiom.pipeline.steps","results":{"primaryType":"org.apache.spark.sql.DataFrame"}},"modifiedDate":{"$date":"2019-11-05T12:15:58.274Z"},"params":[{"type":"text","name":"jdbcOptions","required":false,"parameterType":"org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions"}],"type":"Pipeline"}
{"_id":{"$oid":"5dc167fef8d649ecb358d9bc"},"id":"72dbbfc8-bd1d-4ce4-ab35-28fa8385ea54","category":"InputOutput","creationDate":{"$date":"2019-11-05T12:15:58.280Z"},"description":"This step will load a table from the provided JDBCDataFrameReaderOptions","displayName":"Load table as DataFrame using StepOptions","engineMeta":{"spark":"JDBCSteps.readWithStepOptions","pkg":"com.acxiom.pipeline.steps","results":{"primaryType":"org.apache.spark.sql.DataFrame"}},"modifiedDate":{"$date":"2019-11-05T12:15:58.280Z"},"params":[{"type":"object","name":"jDBCStepsOptions","required":false,"className":"com.acxiom.pipeline.steps.JDBCDataFrameReaderOptions","parameterType":"com.acxiom.pipeline.steps.JDBCDataFrameReaderOptions"}],"type":"Pipeline"}
{"_id":{"$oid":"5dc167fef8d649ecb358d9c0"},"id":"dcc57409-eb91-48c0-975b-ca109ba30195","category":"InputOutput","creationDate":{"$date":"2019-11-05T12:15:58.287Z"},"description":"This step will load a table from the provided jdbc information","displayName":"Load table as DataFrame","engineMeta":{"spark":"JDBCSteps.readWithProperties","pkg":"com.acxiom.pipeline.steps","results":{"primaryType":"org.apache.spark.sql.DataFrame"}},"modifiedDate":{"$date":"2019-11-05T12:15:58.287Z"},"params":[{"type":"text","name":"url","required":false,"parameterType":"String"},{"type":"text","name":"table","required":false,"parameterType":"String"},{"type":"text","name":"predicates","required":false,"parameterType":"List[String]"},{"type":"text","name":"connectionProperties","required":false,"parameterType":"Map[String,String]"}],"type":"Pipeline"}
{"_id":{"$oid":"5dc167fef8d649ecb358d9c4"},"id":"c9fddf52-34b1-4216-a049-10c33ccd24ab","category":"InputOutput","creationDate":{"$date":"2019-11-05T12:15:58.292Z"},"description":"This step will write a DataFrame as a table using JDBCOptions","displayName":"Write DataFrame to table using JDBCOptions","engineMeta":{"spark":"JDBCSteps.writeWithJDBCOptions","pkg":"com.acxiom.pipeline.steps"},"modifiedDate":{"$date":"2019-11-05T12:15:58.292Z"},"params":[{"type":"text","name":"dataFrame","required":false,"parameterType":"org.apache.spark.sql.DataFrame"},{"type":"text","name":"jdbcOptions","required":false,"parameterType":"org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions"},{"type":"text","name":"saveMode","required":false,"parameterType":"String"}],"type":"Pipeline"}
{"_id":{"$oid":"5dc167fef8d649ecb358d9c8"},"id":"77ffcd02-fbd0-4f79-9b35-ac9dc5fb7190","category":"InputOutput","creationDate":{"$date":"2019-11-05T12:15:58.298Z"},"description":"This step will write a DataFrame to a table using the provided properties","displayName":"Write DataFrame to table","engineMeta":{"spark":"JDBCSteps.writeWithProperties","pkg":"com.acxiom.pipeline.steps"},"modifiedDate":{"$date":"2019-11-05T12:15:58.298Z"},"params":[{"type":"text","name":"dataFrame","required":false,"parameterType":"org.apache.spark.sql.DataFrame"},{"type":"text","name":"url","required":false,"parameterType":"String"},{"type":"text","name":"table","required":false,"parameterType":"String"},{"type":"text","name":"connectionProperties","required":false,"parameterType":"Map[String,String]"},{"type":"text","name":"saveMode","required":false,"parameterType":"String"}],"type":"Pipeline"}
{"_id":{"$oid":"5dc167fef8d649ecb358d9cc"},"id":"3d6b77a1-52c2-49ba-99a0-7ec773dac696","category":"InputOutput","creationDate":{"$date":"2019-11-05T12:15:58.301Z"},"description":"This step will write a DataFrame to a table using the provided JDBCDataFrameWriterOptions","displayName":"Write DataFrame to JDBC table","engineMeta":{"spark":"JDBCSteps.writeWithStepOptions","pkg":"com.acxiom.pipeline.steps"},"modifiedDate":{"$date":"2019-11-05T12:15:58.301Z"},"params":[{"type":"text","name":"dataFrame","required":false,"parameterType":"org.apache.spark.sql.DataFrame"},{"type":"object","name":"jDBCStepsOptions","required":false,"className":"com.acxiom.pipeline.steps.JDBCDataFrameWriterOptions","parameterType":"com.acxiom.pipeline.steps.JDBCDataFrameWriterOptions"}],"type":"Pipeline"}
{"_id":{"$oid":"5dc167fef8d649ecb358d9d0"},"id":"219c787a-f502-4efc-b15d-5beeff661fc0","category":"Transforms","creationDate":{"$date":"2019-11-05T12:15:58.303Z"},"description":"This step maps a new dataframe to an existing dataframe to make them compatible","displayName":"Map a DataFrame to an existing DataFrame","engineMeta":{"spark":"TransformationSteps.mapToDestinationDataFrame","pkg":"com.acxiom.pipeline.steps","results":{"primaryType":"org.apache.spark.sql.DataFrame"}},"modifiedDate":{"$date":"2019-11-05T12:15:58.303Z"},"params":[{"type":"text","name":"inputDataFrame","required":false,"parameterType":"org.apache.spark.sql.DataFrame"},{"type":"text","name":"destinationDataFrame","required":false,"parameterType":"org.apache.spark.sql.DataFrame"},{"type":"object","name":"transforms","required":false,"className":"com.acxiom.pipeline.steps.Transformations","parameterType":"com.acxiom.pipeline.steps.Transformations"},{"type":"boolean","name":"addNewColumns","required":false,"parameterType":"Boolean"}],"type":"Pipeline"}
{"_id":{"$oid":"5dc167fef8d649ecb358d9d4"},"id":"8f9c08ea-4882-4265-bac7-2da3e942758f","category":"Transforms","creationDate":{"$date":"2019-11-05T12:15:58.306Z"},"description":"This step maps a new dataframe to a pre-defined spark schema","displayName":"Map a DataFrame to a pre-defined Schema","engineMeta":{"spark":"TransformationSteps.mapDataFrameToSchema","pkg":"com.acxiom.pipeline.steps","results":{"primaryType":"org.apache.spark.sql.DataFrame"}},"modifiedDate":{"$date":"2019-11-05T12:15:58.306Z"},"params":[{"type":"text","name":"inputDataFrame","required":false,"parameterType":"org.apache.spark.sql.DataFrame"},{"type":"object","name":"destinationSchema","required":false,"className":"com.acxiom.pipeline.steps.Schema","parameterType":"com.acxiom.pipeline.steps.Schema"},{"type":"object","name":"transforms","required":false,"className":"com.acxiom.pipeline.steps.Transformations","parameterType":"com.acxiom.pipeline.steps.Transformations"},{"type":"boolean","name":"addNewColumns","required":false,"parameterType":"Boolean"}],"type":"Pipeline"}
{"_id":{"$oid":"5dc167fef8d649ecb358d9d8"},"id":"3ee74590-9131-43e1-8ee8-ad320482a592","category":"Transforms","creationDate":{"$date":"2019-11-05T12:15:58.311Z"},"description":"This step merges two dataframes to create a single dataframe","displayName":"Merge a DataFrame to an existing DataFrame","engineMeta":{"spark":"TransformationSteps.mergeDataFrames","pkg":"com.acxiom.pipeline.steps","results":{"primaryType":"org.apache.spark.sql.DataFrame"}},"modifiedDate":{"$date":"2019-11-05T12:15:58.311Z"},"params":[{"type":"text","name":"inputDataFrame","required":false,"parameterType":"org.apache.spark.sql.DataFrame"},{"type":"text","name":"destinationDataFrame","required":false,"parameterType":"org.apache.spark.sql.DataFrame"},{"type":"object","name":"transforms","required":false,"className":"com.acxiom.pipeline.steps.Transformations","parameterType":"com.acxiom.pipeline.steps.Transformations"},{"type":"boolean","name":"addNewColumns","required":false,"parameterType":"Boolean"}],"type":"Pipeline"}
{"_id":{"$oid":"5dc167fef8d649ecb358d9dc"},"id":"ac3dafe4-e6ee-45c9-8fc6-fa7f918cf4f2","category":"Transforms","creationDate":{"$date":"2019-11-05T12:15:58.317Z"},"description":"This step transforms existing columns and/or adds new columns to an existing dataframe using expressions provided","displayName":"Modify or Create Columns using Transforms Provided","engineMeta":{"spark":"TransformationSteps.applyTransforms","pkg":"com.acxiom.pipeline.steps","results":{"primaryType":"org.apache.spark.sql.DataFrame"}},"modifiedDate":{"$date":"2019-11-05T12:15:58.317Z"},"params":[{"type":"text","name":"dataFrame","required":false,"parameterType":"org.apache.spark.sql.DataFrame"},{"type":"object","name":"transforms","required":false,"className":"com.acxiom.pipeline.steps.Transformations","parameterType":"com.acxiom.pipeline.steps.Transformations"}],"type":"Pipeline"}
{"_id":{"$oid":"5dc167fef8d649ecb358d9e0"},"id":"fa0fcabb-d000-4a5e-9144-692bca618ddb","category":"Transforms","creationDate":{"$date":"2019-11-05T12:15:58.319Z"},"description":"This step will filter a dataframe based on the where expression provided","displayName":"Filter a DataFrame","engineMeta":{"spark":"TransformationSteps.applyFilter","pkg":"com.acxiom.pipeline.steps","results":{"primaryType":"org.apache.spark.sql.DataFrame"}},"modifiedDate":{"$date":"2019-11-05T12:15:58.319Z"},"params":[{"type":"text","name":"dataFrame","required":false,"parameterType":"org.apache.spark.sql.DataFrame"},{"type":"text","name":"expression","required":false,"parameterType":"String"}],"type":"Pipeline"}
{"_id":{"$oid":"5dc167fef8d649ecb358d9e4"},"id":"a981080d-714c-4d36-8b09-d95842ec5655","category":"Transforms","creationDate":{"$date":"2019-11-05T12:15:58.322Z"},"description":"This step will standardize columns names on existing dataframe","displayName":"Standardize Column Names on a DataFrame","engineMeta":{"spark":"TransformationSteps.standardizeColumnNames","pkg":"com.acxiom.pipeline.steps","results":{"primaryType":"org.apache.spark.sql.DataFrame"}},"modifiedDate":{"$date":"2019-11-05T12:15:58.322Z"},"params":[{"type":"text","name":"dataFrame","required":false,"parameterType":"org.apache.spark.sql.DataFrame"}],"type":"Pipeline"}
{"_id":{"$oid":"5dc167fef8d649ecb358d9e8"},"id":"541c4f7d-3524-4d53-bbd9-9f2cfd9d1bd1","category":"Query","creationDate":{"$date":"2019-11-05T12:15:58.325Z"},"description":"This step stores an existing dataframe to a TempView to be used in future queries in the session","displayName":"Save a Dataframe to a TempView","engineMeta":{"spark":"QuerySteps.dataFrameToTempView","pkg":"com.acxiom.pipeline.steps","results":{"primaryType":"String"}},"modifiedDate":{"$date":"2019-11-05T12:15:58.325Z"},"params":[{"type":"text","name":"dataFrame","required":false,"parameterType":"org.apache.spark.sql.DataFrame"},{"type":"text","name":"viewName","required":false,"parameterType":"String"}],"type":"Pipeline"}
{"_id":{"$oid":"5dc167fef8d649ecb358d9ec"},"id":"71b71ef3-eaa7-4a1f-b3f3-603a1a54846d","category":"Query","creationDate":{"$date":"2019-11-05T12:15:58.329Z"},"description":"This step runs a SQL statement against existing TempViews from this session and returns a new TempView","displayName":"Create a TempView from a Query","engineMeta":{"spark":"QuerySteps.queryToTempView","pkg":"com.acxiom.pipeline.steps","results":{"primaryType":"String"}},"modifiedDate":{"$date":"2019-11-05T12:15:58.329Z"},"params":[{"type":"script","name":"query","required":false,"language":"sql","className":"String"},{"type":"text","name":"variableMap","required":false,"parameterType":"Map[String,String]"},{"type":"text","name":"viewName","required":false,"parameterType":"String"}],"type":"Pipeline"}
{"_id":{"$oid":"5dc167fef8d649ecb358d9f0"},"id":"61378ed6-8a4f-4e6d-9c92-6863c9503a54","category":"Query","creationDate":{"$date":"2019-11-05T12:15:58.332Z"},"description":"This step runs a SQL statement against existing TempViews from this session and returns a new DataFrame","displayName":"Create a DataFrame from a Query","engineMeta":{"spark":"QuerySteps.queryToDataFrame","pkg":"com.acxiom.pipeline.steps","results":{"primaryType":"org.apache.spark.sql.DataFrame"}},"modifiedDate":{"$date":"2019-11-05T12:15:58.332Z"},"params":[{"type":"script","name":"query","required":false,"language":"sql","className":"String"},{"type":"text","name":"variableMap","required":false,"parameterType":"Map[String,String]"}],"type":"Pipeline"}
{"_id":{"$oid":"5dc167fef8d649ecb358d9f4"},"id":"57b0e491-e09b-4428-aab2-cebe1f217eda","category":"Query","creationDate":{"$date":"2019-11-05T12:15:58.335Z"},"description":"This step pulls an existing TempView from this session into a new DataFrame","displayName":"Create a DataFrame from an Existing TempView","engineMeta":{"spark":"QuerySteps.tempViewToDataFrame","pkg":"com.acxiom.pipeline.steps","results":{"primaryType":"org.apache.spark.sql.DataFrame"}},"modifiedDate":{"$date":"2019-11-05T12:15:58.335Z"},"params":[{"type":"text","name":"viewName","required":false,"parameterType":"String"}],"type":"Pipeline"}
{"_id":{"$oid":"5dc167fef8d649ecb358d9f8"},"id":"648f27aa-6e3b-44ed-a093-bc284783731b","category":"Query","creationDate":{"$date":"2019-11-05T12:15:58.338Z"},"description":"This step runs a SQL statement against an existing DataFrame from this session and returns a new TempView","displayName":"Create a TempView from a DataFrame Query","engineMeta":{"spark":"QuerySteps.dataFrameQueryToTempView","pkg":"com.acxiom.pipeline.steps","results":{"primaryType":"String"}},"modifiedDate":{"$date":"2019-11-05T12:15:58.338Z"},"params":[{"type":"text","name":"dataFrame","required":false,"parameterType":"org.apache.spark.sql.DataFrame"},{"type":"script","name":"query","required":false,"language":"sql","className":"String"},{"type":"text","name":"variableMap","required":false,"parameterType":"Map[String,String]"},{"type":"text","name":"inputViewName","required":false,"parameterType":"String"},{"type":"text","name":"outputViewName","required":false,"parameterType":"String"}],"type":"Pipeline"}
{"_id":{"$oid":"5dc167fef8d649ecb358d9fc"},"id":"dfb8a387-6245-4b1c-ae6c-94067eb83962","category":"Query","creationDate":{"$date":"2019-11-05T12:15:58.345Z"},"description":"This step runs a SQL statement against an existing DataFrame from this session and returns a new DataFrame","displayName":"Create a DataFrame from a DataFrame Query","engineMeta":{"spark":"QuerySteps.dataFrameQueryToDataFrame","pkg":"com.acxiom.pipeline.steps","results":{"primaryType":"org.apache.spark.sql.DataFrame"}},"modifiedDate":{"$date":"2019-11-05T12:15:58.345Z"},"params":[{"type":"text","name":"dataFrame","required":false,"parameterType":"org.apache.spark.sql.DataFrame"},{"type":"script","name":"query","required":false,"language":"sql","className":"String"},{"type":"text","name":"variableMap","required":false,"parameterType":"Map[String,String]"},{"type":"text","name":"inputViewName","required":false,"parameterType":"String"}],"type":"Pipeline"}
{"_id":{"$oid":"5dc167fef8d649ecb358da00"},"id":"c88de095-14e0-4c67-8537-0325127e2bd2","category":"Query","creationDate":{"$date":"2019-11-05T12:15:58.351Z"},"description":"This step will cache an existing TempView","displayName":"Cache an exising TempView","engineMeta":{"spark":"QuerySteps.cacheTempView","pkg":"com.acxiom.pipeline.steps","results":{"primaryType":"org.apache.spark.sql.DataFrame"}},"modifiedDate":{"$date":"2019-11-05T12:15:58.351Z"},"params":[{"type":"text","name":"viewName","required":false,"parameterType":"String"}],"type":"Pipeline"}
{"_id":{"$oid":"5dc167fef8d649ecb358da04"},"id":"0342654c-2722-56fe-ba22-e342169545af","category":"InputOutput","creationDate":{"$date":"2019-11-05T12:15:58.353Z"},"description":"Copy the contents of the source path to the destination path. This function will call connect on both FileManagers.","displayName":"Copy source contents to destination","engineMeta":{"spark":"FileManagerSteps.copy","pkg":"com.acxiom.pipeline.steps","results":{"primaryType":"com.acxiom.pipeline.steps.CopyResults"}},"modifiedDate":{"$date":"2019-11-05T12:15:58.353Z"},"params":[{"type":"text","name":"srcFS","required":false,"parameterType":"com.acxiom.pipeline.fs.FileManager"},{"type":"text","name":"srcPath","required":false,"parameterType":"String"},{"type":"text","name":"destFS","required":false,"parameterType":"com.acxiom.pipeline.fs.FileManager"},{"type":"text","name":"destPath","required":false,"parameterType":"String"}],"type":"Pipeline"}
{"_id":{"$oid":"5dc167fef8d649ecb358da08"},"id":"c40169a3-1e77-51ab-9e0a-3f24fb98beef","category":"InputOutput","creationDate":{"$date":"2019-11-05T12:15:58.356Z"},"description":"Copy the contents of the source path to the destination path using buffer sizes. This function will call connect on both FileManagers.","displayName":"Copy source contents to destination with buffering","engineMeta":{"spark":"FileManagerSteps.copy","pkg":"com.acxiom.pipeline.steps","results":{"primaryType":"com.acxiom.pipeline.steps.CopyResults"}},"modifiedDate":{"$date":"2019-11-05T12:15:58.356Z"},"params":[{"type":"text","name":"srcFS","required":false,"parameterType":"com.acxiom.pipeline.fs.FileManager"},{"type":"text","name":"srcPath","required":false,"parameterType":"String"},{"type":"text","name":"destFS","required":false,"parameterType":"com.acxiom.pipeline.fs.FileManager"},{"type":"text","name":"destPath","required":false,"parameterType":"String"},{"type":"text","name":"inputBufferSize","required":false,"parameterType":"Int"},{"type":"text","name":"outputBufferSize","required":false,"parameterType":"Int"}],"type":"Pipeline"}
{"_id":{"$oid":"5dc167fef8d649ecb358da0c"},"id":"f5a24db0-e91b-5c88-8e67-ab5cff09c883","category":"InputOutput","creationDate":{"$date":"2019-11-05T12:15:58.360Z"},"description":"Copy the contents of the source path to the destination path using full buffer sizes. This function will call connect on both FileManagers.","displayName":"Buffered file copy","engineMeta":{"spark":"FileManagerSteps.copy","pkg":"com.acxiom.pipeline.steps","results":{"primaryType":"com.acxiom.pipeline.steps.CopyResults"}},"modifiedDate":{"$date":"2019-11-05T12:15:58.360Z"},"params":[{"type":"text","name":"srcFS","required":false,"parameterType":"com.acxiom.pipeline.fs.FileManager"},{"type":"text","name":"srcPath","required":false,"parameterType":"String"},{"type":"text","name":"destFS","required":false,"parameterType":"com.acxiom.pipeline.fs.FileManager"},{"type":"text","name":"destPath","required":false,"parameterType":"String"},{"type":"text","name":"inputBufferSize","required":false,"parameterType":"Int"},{"type":"text","name":"outputBufferSize","required":false,"parameterType":"Int"},{"type":"text","name":"copyBufferSize","required":false,"parameterType":"Int"}],"type":"Pipeline"}
{"_id":{"$oid":"5dc167fef8d649ecb358da10"},"id":"3d1e8519-690c-55f0-bd05-1e7b97fb6633","category":"InputOutput","creationDate":{"$date":"2019-11-05T12:15:58.370Z"},"description":"Disconnects a FileManager from the underlying file system","displayName":"Disconnect a FileManager","engineMeta":{"spark":"FileManagerSteps.disconnectFileManager","pkg":"com.acxiom.pipeline.steps"},"modifiedDate":{"$date":"2019-11-05T12:15:58.370Z"},"params":[{"type":"text","name":"fileManager","required":false,"parameterType":"com.acxiom.pipeline.fs.FileManager"}],"type":"Pipeline"}
{"_id":{"$oid":"5dc167fef8d649ecb358da14"},"id":"9d467cb0-8b3d-40a0-9ccd-9cf8c5b6cb38","category":"InputOutput","creationDate":{"$date":"2019-11-05T12:15:58.376Z"},"description":"Simple function to generate the SFTPFileManager for the remote SFTP file system","displayName":"Create SFTP FileManager","engineMeta":{"spark":"SFTPSteps.createFileManager","pkg":"com.acxiom.pipeline.steps","results":{"primaryType":"com.acxiom.pipeline.fs.SFTPFileManager"}},"modifiedDate":{"$date":"2019-11-05T12:15:58.376Z"},"params":[{"type":"text","name":"hostName","required":false,"parameterType":"String"},{"type":"text","name":"username","required":false,"parameterType":"String"},{"type":"text","name":"password","required":false,"parameterType":"String"},{"type":"text","name":"port","required":false,"parameterType":"Int"},{"type":"boolean","name":"strictHostChecking","required":false,"parameterType":"Boolean"}],"type":"Pipeline"}
{"_id":{"$oid":"5dc167fef8d649ecb358da18"},"id":"22fcc0e7-0190-461c-a999-9116b77d5919","category":"InputOutput","creationDate":{"$date":"2019-11-05T12:15:58.379Z"},"description":"This step will build a DataFrameReader object that can be used to read a file into a dataframe","displayName":"Build a DataFrameReader Object","engineMeta":{"spark":"DataFrameSteps.getDataFrameReader","pkg":"com.acxiom.pipeline.steps","results":{"primaryType":"org.apache.spark.sql.DataFrameReader"}},"modifiedDate":{"$date":"2019-11-05T12:15:58.379Z"},"params":[{"type":"object","name":"dataFrameReaderOptions","required":false,"className":"com.acxiom.pipeline.steps.DataFrameReaderOptions","parameterType":"com.acxiom.pipeline.steps.DataFrameReaderOptions"}],"type":"Pipeline"}
{"_id":{"$oid":"5dc167fef8d649ecb358da1c"},"id":"e023fc14-6cb7-44cb-afce-7de01d5cdf00","category":"InputOutput","creationDate":{"$date":"2019-11-05T12:15:58.383Z"},"description":"This step will build a DataFrameWriter object that can be used to write a file into a dataframe","displayName":"Build a DataFrameWriter Object","engineMeta":{"spark":"DataFrameSteps.getDataFrameWriter","pkg":"com.acxiom.pipeline.steps","results":{"primaryType":"org.apache.spark.sql.DataFrameWriter[org.apache.spark.sql.Row]"}},"modifiedDate":{"$date":"2019-11-05T12:15:58.383Z"},"params":[{"type":"text","name":"dataFrame","required":false,"parameterType":"org.apache.spark.sql.DataFrame"},{"type":"object","name":"options","required":false,"className":"com.acxiom.pipeline.steps.DataFrameWriterOptions","parameterType":"com.acxiom.pipeline.steps.DataFrameWriterOptions"}],"type":"Pipeline"}
{"_id":{"$oid":"5dc167fef8d649ecb358da20"},"id":"5e0358a0-d567-5508-af61-c35a69286e4e","category":"Scripting","creationDate":{"$date":"2019-11-05T12:15:58.387Z"},"description":"Executes a script and returns the result","displayName":"Javascript Step","engineMeta":{"spark":"JavascriptSteps.processScript","pkg":"com.acxiom.pipeline.steps","results":{"primaryType":"com.acxiom.pipeline.PipelineStepResponse"}},"modifiedDate":{"$date":"2019-11-05T12:15:58.387Z"},"params":[{"type":"script","name":"script","required":false,"language":"javascript","className":"String"}],"type":"Pipeline"}
{"_id":{"$oid":"5dc167fef8d649ecb358da24"},"id":"570c9a80-8bd1-5f0c-9ae0-605921fe51e2","category":"Scripting","creationDate":{"$date":"2019-11-05T12:15:58.393Z"},"description":"Executes a script and returns the result","displayName":"Javascript Step with additional object provided","engineMeta":{"spark":"JavascriptSteps.processScriptWithValue","pkg":"com.acxiom.pipeline.steps","results":{"primaryType":"com.acxiom.pipeline.PipelineStepResponse"}},"modifiedDate":{"$date":"2019-11-05T12:15:58.393Z"},"params":[{"type":"script","name":"script","required":false,"language":"javascript","className":"String"},{"type":"text","name":"value","required":false,"parameterType":"Any"}],"type":"Pipeline"}
